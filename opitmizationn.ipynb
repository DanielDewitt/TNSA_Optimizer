{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T12:33:48.478049Z",
     "start_time": "2024-08-22T12:33:47.253261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a .npy file: .DS_Store\n",
      "(tensor([ 0.9055,  0.9055,  0.9055, -0.4136, -1.5403,  0.8123,  1.0000],\n",
      "       requires_grad=True), tensor([-1.1203, -1.2076,  0.9738, -0.9646, -0.9335,  0.8144],\n",
      "       grad_fn=<ToCopyBackward0>))\n",
      "1000\n",
      "tensor([[ 0.8688,  0.8688,  0.8688,  ...,  0.2199,  0.9117,  1.0000],\n",
      "        [ 0.8446,  0.8446,  0.8446,  ..., -1.4352,  0.8850,  1.0000],\n",
      "        [ 0.8787,  0.8787,  0.8787,  ...,  0.3570,  0.8127,  1.0000],\n",
      "        ...,\n",
      "        [ 0.9208,  0.9208,  0.9208,  ..., -0.6074,  0.8742,  1.0000],\n",
      "        [ 0.8883,  0.8883,  0.8883,  ..., -1.3187,  0.8104,  1.0000],\n",
      "        [ 0.8406,  0.8406,  0.8406,  ..., -1.4023,  0.9456,  1.0000]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import surrogates\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "input_data = surrogates.dir_to_dataset_list(\"Archive/LIGHTbeams/Trial0_Training_Data/training/240618\")\n",
    "\n",
    "file = open(\"Archive/240613_model_tuning/model005/starting_data/model005_rev7_cpu.pickle\", \"rb\")\n",
    "optimizer_model = pickle.load(file)\n",
    "file.close()\n",
    "optimizer_model.requires_grad_(False)\n",
    "\n",
    "input_dataloader = torch.utils.data.DataLoader(input_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "data_iter = iter(input_dataloader)\n",
    "input_batch, output_batch = next(data_iter)\n",
    "input_batch.requires_grad_(True)\n",
    "\n",
    "print(input_batch.__len__())\n",
    "print(input_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8974,  0.8974,  0.8974, -0.4180, -0.7581, -0.2290,  0.9200],\n",
      "       requires_grad=True)\n",
      "[0.07106251893096134, 0.07115837465863079, 2.6148913472005828e-08, 10369002.57741696, 10754515.254938947, 64026025.65215367, 14.999999999999964]\n"
     ]
    }
   ],
   "source": [
    "print(input_data.__getitem__(0)[0])\n",
    "print(input_data.out_max_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T20:14:19.084519Z",
     "start_time": "2024-06-17T20:14:19.073969Z"
    }
   },
   "id": "51440f1f70b9a51f",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#print(optimizer_model(input_data.__getitem__(700)[0]))\n",
    "\n",
    "\n",
    "def prototype_solenoid_optimization(input_batch, model, strom = 0.5, l_drift = 4.9):\n",
    "    \n",
    "    aperture = 0.02\n",
    "    input_batch.clone()\n",
    "    input_tensor_0 = input_batch[:, 0:6]\n",
    "\n",
    "    #optimizer_model.float()\n",
    "    #input_tensor_0.float()\n",
    "\n",
    "    strom_tensor = torch.ones(input_tensor_0.__len__(), dtype=torch.float32, requires_grad=True)*strom\n",
    "    strom_resh = strom_tensor.view(-1, 1)\n",
    "    #print(strom_tensor)\n",
    "\n",
    "    norm_tensor = torch.tensor([0.07119202565482073, 0.07120724187081795, 2.615718890351589e-08, 10955187.108948682, 10969527.532464387, 64027733.68489398], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    denormalized_output = ((optimizer_model(torch.cat((input_tensor_0, strom_resh), dim=1))+1)/2)*norm_tensor\n",
    "    #denormalized_output_filtered_x = denormalized_output[denormalized_output[:,0] < aperture]\n",
    "    #denormalized_output_filtered_y = denormalized_output[denormalized_output[:,1] < aperture]\n",
    "\n",
    "    #t_drift = l_drift/denormalized_output[:,5]\n",
    "    t_drift = l_drift/denormalized_output[:,5]\n",
    "\n",
    "    x_drift = t_drift*denormalized_output[:, 3]\n",
    "    y_drift = t_drift*denormalized_output[:, 4]\n",
    "\n",
    "    x_std = torch.std(x_drift)\n",
    "    y_std = torch.std(y_drift)\n",
    "\n",
    "    loss = torch.sqrt(x_std**2 + y_std**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T11:33:12.338578Z",
     "start_time": "2024-06-18T11:33:12.322427Z"
    }
   },
   "id": "afbe7e796da8ddc0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SurrogateOptimizer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, par, input_batch, surrogate_model):\n",
    "        super().__init__()\n",
    "        # register set of parameter:\n",
    "        self.input_batch = input_batch\n",
    "        self.surrogate_model = surrogate_model\n",
    "        self.register_parameter('par', torch.nn.Parameter(par, requires_grad=True))\n",
    "\n",
    "    def forward(self):\n",
    "        # create lattice given quad strengths in k_set:\n",
    "        loss = prototype_solenoid_optimization(self.input_batch, self.surrogate_model, self.par, torch.tensor(4.9, requires_grad=True))\n",
    "        #self.par.register_hook(lambda grad: print(grad))\n",
    "        #self.input_batch.register_hook(lambda grad_in: print(grad_in))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T11:33:13.566077Z",
     "start_time": "2024-06-18T11:33:13.545578Z"
    }
   },
   "id": "e3149eea0a87fb5d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(model, training_iter, alpha=0.1):\n",
    "    history_param = [None] * training_iter  # list to save params\n",
    "    history_loss = [None] * training_iter  # list to save loss\n",
    "\n",
    "\n",
    "\n",
    "    # Use PyTorch Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(recurse=False), alpha)\n",
    "    \n",
    "        # print the trainable parameters\n",
    "    #for param in model.named_parameters():\n",
    "    #    print(f'{param[0]} : {param[1]}')\n",
    "\n",
    "    for i in range(training_iter):\n",
    "\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Calc loss and backprop gradients\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        loss = model()  # loss is just O.F.\n",
    "        loss.backward()  # gradient#\n",
    "        optimizer.step()\n",
    "\n",
    "        # print info:\n",
    "        if i % 10 == 0:  # print each 100 steps\n",
    "            print('Iter %d/%d - Loss: %.5f ' % (\n",
    "                i + 1, training_iter, loss\n",
    "            ))\n",
    "\n",
    "        for param in model.parameters():\n",
    "            history_param[i] = param.data.detach().numpy().copy()\n",
    "            history_loss[i] = loss.detach().numpy().copy()\n",
    "\n",
    "        # optimization step\n",
    "\n",
    "    # returns params and loss for every iteration\n",
    "    return np.asarray(history_param), np.asarray(history_loss)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T11:33:14.693566Z",
     "start_time": "2024-06-18T11:33:14.674393Z"
    }
   },
   "id": "b84e03ac3e273921",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 0.11160 \n",
      "Iter 11/200 - Loss: 0.11048 \n",
      "Iter 21/200 - Loss: 0.10953 \n",
      "Iter 31/200 - Loss: 0.10969 \n",
      "Iter 41/200 - Loss: 0.10970 \n",
      "Iter 51/200 - Loss: 0.10953 \n",
      "Iter 61/200 - Loss: 0.10955 \n",
      "Iter 71/200 - Loss: 0.10954 \n",
      "Iter 81/200 - Loss: 0.10953 \n",
      "Iter 91/200 - Loss: 0.10953 \n",
      "Iter 101/200 - Loss: 0.10953 \n",
      "Iter 111/200 - Loss: 0.10953 \n",
      "Iter 121/200 - Loss: 0.10953 \n",
      "Iter 131/200 - Loss: 0.10953 \n",
      "Iter 141/200 - Loss: 0.10953 \n",
      "Iter 151/200 - Loss: 0.10953 \n",
      "Iter 161/200 - Loss: 0.10953 \n",
      "Iter 171/200 - Loss: 0.10953 \n",
      "Iter 181/200 - Loss: 0.10953 \n",
      "Iter 191/200 - Loss: 0.10953 \n"
     ]
    }
   ],
   "source": [
    "strom = torch.tensor(0.8, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "surrogate_optimizer_model = SurrogateOptimizer(strom, input_batch, optimizer_model)\n",
    "\n",
    "params, loss = train_model(surrogate_optimizer_model, 200, .1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T11:33:39.925005Z",
     "start_time": "2024-06-18T11:33:15.709134Z"
    }
   },
   "id": "c021057226b76fae",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8208, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(strom)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T11:31:01.746217Z",
     "start_time": "2024-06-18T11:31:01.744219Z"
    }
   },
   "id": "ce222160d8a697da",
   "execution_count": 178
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6a593631bd6b73ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
